{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import jumanji\n",
    "from jumanji.wrappers import AutoResetWrapper\n",
    "import dm_env\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_observation_element(observation_spec, observation, key):\n",
    "    \"\"\"\n",
    "    Helper to return a specific observation element based on the observation_spec structure.\n",
    "\n",
    "    Args:\n",
    "        observation_spec (dict): The dictionary specifying the observation structure.\n",
    "        observation (tuple or list): The actual observation values in the same order as observation_spec.\n",
    "        key (str): The name of the element to extract.\n",
    "\n",
    "    Returns:\n",
    "        The corresponding element from observation.\n",
    "    \"\"\"\n",
    "    keys = list(observation_spec.keys())  # Get the ordered keys from observation_spec\n",
    "    index = keys.index(key)  # Find the index of the requested key\n",
    "    return observation[index]  # Return the corresponding value from observation\n",
    "\n",
    "\n",
    "def plot_learning_curve(list_of_episode_returns):\n",
    "    \"\"\"Plot the learning curve.\"\"\"\n",
    "    plt.figure(figsize=(7, 5))\n",
    "\n",
    "    def moving_average(x, w):\n",
    "        return np.convolve(x, np.ones(w), \"valid\") / w\n",
    "\n",
    "    smoothed_returns = moving_average(list_of_episode_returns, 30)\n",
    "    plt.plot(smoothed_returns)\n",
    "\n",
    "    plt.xlabel(\"Average episode returns\")\n",
    "    plt.xlabel(\"Number of episodes\")\n",
    "\n",
    "    ax = plt.gca()\n",
    "    ax.spines[\"left\"].set_visible(True)\n",
    "    ax.spines[\"bottom\"].set_visible(True)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.xaxis.set_ticks_position(\"bottom\")\n",
    "    ax.yaxis.set_ticks_position(\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def softmax_policy(parameters, key, obs):\n",
    "    \"\"\"Sample action from a softmax policy.\"\"\"\n",
    "    _, p = network(parameters, obs)\n",
    "    return jax.random.categorical(key, p)\n",
    "\n",
    "\n",
    "def network(params, observation):\n",
    "    # Implement forward pass here\n",
    "    w = params[\"w\"]\n",
    "    b = params[\"b\"]\n",
    "    # these are theta (vector) -- policy fn params\n",
    "    w_p = params[\"w_p\"]\n",
    "    b_p = params[\"b_p\"]\n",
    "    # these are w (vector) -- value fn params (just overloaded notation bc they're linear params so hence the w name for weights)\n",
    "    w_v = params[\"w_v\"]\n",
    "    b_v = params[\"b_v\"]\n",
    "\n",
    "    flat_observation = observation.flatten()\n",
    "    h = jax.nn.relu(flat_observation.dot(w) + b)  # hidden representation\n",
    "    # print(flat_observation.shape,w.shape,h.shape)\n",
    "    p = h.dot(w_p) + b_p\n",
    "    # print(h.shape,w_p.shape,p.shape)\n",
    "    v = h.dot(w_v) + b_v\n",
    "    return v, p\n",
    "\n",
    "\n",
    "def create_parameters(rng_key, observation):\n",
    "    # Returns a dictionary with the desired parameters for the network\n",
    "    params = {}\n",
    "    rng_key, param_key = jax.random.split(rng_key)\n",
    "    params[\"w\"] = jax.random.truncated_normal(\n",
    "        param_key, -1, 1, (jnp.size(observation), jnp.size(observation))\n",
    "    )\n",
    "    rng_key, param_key = jax.random.split(rng_key)\n",
    "    params[\"b\"] = jax.random.truncated_normal(param_key, -1, 1, (jnp.size(observation),))\n",
    "    rng_key, param_key = jax.random.split(rng_key)\n",
    "    params[\"w_p\"] = jax.random.truncated_normal(param_key, -1, 1, (jnp.size(observation), 3))\n",
    "    rng_key, param_key = jax.random.split(rng_key)\n",
    "    params[\"b_p\"] = jax.random.truncated_normal(param_key, -1, 1, (3,))\n",
    "    rng_key, param_key = jax.random.split(rng_key)\n",
    "    params[\"w_v\"] = jax.random.truncated_normal(param_key, -1, 1, (jnp.size(observation), 1))\n",
    "    rng_key, param_key = jax.random.split(rng_key)\n",
    "    params[\"b_v\"] = jax.random.truncated_normal(param_key, -1, 1, (1,))\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_TD(parameters, obs_tm1, r_t, discount_t, obs_t):\n",
    "    v_t, _ = network(parameters, obs_t)\n",
    "    v_tm1, p_tm1 = network(parameters, obs_tm1)\n",
    "\n",
    "    # Calculate the TD error\n",
    "    q_pi = r_t + discount_t * v_t\n",
    "    TD_err = q_pi - v_tm1\n",
    "    grad = jax.lax.stop_gradient(TD_err[0]) * v_tm1[0]\n",
    "    return grad\n",
    "\n",
    "\n",
    "def value_update(parameters, obs_tm1, a_tm1, r_t, discount_t, obs_t):\n",
    "    value_grads = jax.grad(value_TD)(parameters, obs_tm1, r_t, discount_t, obs_t)\n",
    "\n",
    "    return value_grads\n",
    "\n",
    "\n",
    "def policy_TD(parameters, obs_tm1, a_tm1, r_t, discount_t, obs_t):\n",
    "    # Get the current and previous state value and policy logits\n",
    "    v_t, p_t = network(parameters, obs_t)\n",
    "    v_tm1, p_tm1 = network(parameters, obs_tm1)\n",
    "    # Compute the log probability of the action a_tm1\n",
    "    log_pi = jax.nn.log_softmax(p_tm1)\n",
    "    log_pi_a_tm1 = log_pi[a_tm1]\n",
    "\n",
    "    # Calculate the TD error\n",
    "    q_pi = r_t + discount_t * v_t\n",
    "    TD_err = q_pi - v_tm1\n",
    "\n",
    "    return log_pi_a_tm1 * jax.lax.stop_gradient(TD_err[0])\n",
    "\n",
    "\n",
    "def policy_gradient(parameters, obs_tm1, a_tm1, r_t, discount_t, obs_t):\n",
    "    policy_grads = jax.grad(policy_TD)(parameters, obs_tm1, a_tm1, r_t, discount_t, obs_t)\n",
    "\n",
    "    return policy_grads\n",
    "\n",
    "\n",
    "def opt_init(parameters):\n",
    "    mu = jax.tree_map(jnp.zeros_like, parameters)\n",
    "    nu = jax.tree_map(jnp.ones_like, parameters)\n",
    "    b1 = 0.9\n",
    "    b2 = 0.999\n",
    "    epsilon = 1e-4\n",
    "    alpha = 0.003\n",
    "    # alpha = 5e-4\n",
    "    opt_state = (alpha, b1, b2, epsilon, mu, nu)\n",
    "    return opt_state\n",
    "\n",
    "\n",
    "def opt_update(grads, opt_state):\n",
    "    alpha, b1, b2, epsilon, mu, nu = opt_state\n",
    "    mu = jax.tree_map(lambda m, g: (1 - b1) * g + b1 * m, mu, grads)\n",
    "    nu = jax.tree_map(lambda n, g: (1 - b2) * (g**2) + b2 * n, nu, grads)\n",
    "    updates = jax.tree_map(lambda m, n: alpha * (m / (epsilon + jnp.sqrt(n))), mu, nu)\n",
    "    opt_state = (alpha, b1, b2, epsilon, mu, nu)\n",
    "    return updates, opt_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def compute_gradient(parameters, obs_tm1, a_tm1, r_t, discount_t, obs_t):\n",
    "    pgrads = policy_gradient(parameters, obs_tm1, a_tm1, r_t, discount_t, obs_t)\n",
    "    td_update = value_update(parameters, obs_tm1, a_tm1, r_t, discount_t, obs_t)\n",
    "    return jax.tree_map(lambda pg, td: pg + td, pgrads, td_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HELLO\n",
      "dict\n",
      "bounded\n",
      "discrete\n",
      "bounded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nz/gfmljb2n1tzf894yqww7gltm0000gp/T/ipykernel_72193/2235839743.py:40: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  mu = jax.tree_map(jnp.zeros_like, parameters)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 2500 episodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 50/2500 [00:02<01:44, 23.52it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m rng, policy_rng \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39msplit(rng)\n\u001b[1;32m     37\u001b[0m a_tm1 \u001b[38;5;241m=\u001b[39m softmax_policy(parameters, policy_rng, obs_tm1)\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m timestep\u001b[38;5;241m.\u001b[39mlast():\n\u001b[1;32m     40\u001b[0m   \u001b[38;5;66;03m# Step environment.\u001b[39;00m\n\u001b[1;32m     41\u001b[0m   new_timestep \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;28mint\u001b[39m(a_tm1))\n\u001b[1;32m     43\u001b[0m   \u001b[38;5;66;03m# Sample action from agent policy.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/reinf/lib/python3.11/site-packages/jax/_src/array.py:295\u001b[0m, in \u001b[0;36mArrayImpl.__bool__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__bool__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    294\u001b[0m   core\u001b[38;5;241m.\u001b[39mcheck_bool_conversion(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 295\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value)\n",
      "File \u001b[0;32m~/miniconda3/envs/reinf/lib/python3.11/site-packages/jax/_src/profiler.py:333\u001b[0m, in \u001b[0;36mannotate_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    332\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m TraceAnnotation(name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdecorator_kwargs):\n\u001b[0;32m--> 333\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    334\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m wrapper\n",
      "File \u001b[0;32m~/miniconda3/envs/reinf/lib/python3.11/site-packages/jax/_src/array.py:629\u001b[0m, in \u001b[0;36mArrayImpl._value\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_npy_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_fully_replicated:\n\u001b[0;32m--> 629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_npy_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_single_device_array_to_np_array()\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_npy_value\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mwriteable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_npy_value)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = jumanji.make(\"Snake-v1\")  # Create a Snake environment\n",
    "env = AutoResetWrapper(env)  # Automatically reset the environment when an episode terminates\n",
    "env = jumanji.wrappers.JumanjiToDMEnvWrapper(env)\n",
    "\n",
    "# Experiment configs.\n",
    "train_episodes = 2500\n",
    "discount_factor = 0.99\n",
    "\n",
    "\n",
    "# Build and initialize network.\n",
    "rng = jax.random.PRNGKey(44)\n",
    "rng, init_rng = jax.random.split(rng)\n",
    "sample_input = env.observation_spec()[\"grid\"].generate_value()\n",
    "parameters = create_parameters(init_rng, sample_input)\n",
    "\n",
    "# Initialize optimizer state.\n",
    "opt_state = opt_init(parameters)\n",
    "\n",
    "\n",
    "# Apply updates\n",
    "def apply_updates(params, updates):\n",
    "    return jax.tree_map(lambda p, u: p + u, params, updates)\n",
    "\n",
    "\n",
    "# Jit.\n",
    "opt_update = jax.jit(opt_update)\n",
    "apply_updates = jax.jit(apply_updates)\n",
    "\n",
    "print(f\"Training agent for {train_episodes} episodes...\")\n",
    "all_episode_returns = []\n",
    "\n",
    "for _ in tqdm(range(train_episodes)):\n",
    "    episode_return = 0.0\n",
    "    timestep = env.reset()\n",
    "    obs_tm1 = timestep.observation[0]\n",
    "\n",
    "    # Sample initial action.\n",
    "    rng, policy_rng = jax.random.split(rng)\n",
    "    a_tm1 = softmax_policy(parameters, policy_rng, obs_tm1)\n",
    "\n",
    "    while not timestep.last():\n",
    "        # Step environment.\n",
    "        new_timestep = env.step(int(a_tm1))\n",
    "\n",
    "        # Sample action from agent policy.\n",
    "        rng, policy_rng = jax.random.split(rng)\n",
    "        a_t = softmax_policy(parameters, policy_rng, new_timestep.observation[0])\n",
    "\n",
    "        # Update params.\n",
    "        r_t = new_timestep.reward\n",
    "        discount_t = discount_factor * new_timestep.discount\n",
    "        dJ_dtheta = compute_gradient(\n",
    "            parameters, obs_tm1, a_tm1, r_t, discount_t, new_timestep.observation[0]\n",
    "        )\n",
    "        updates, opt_state = opt_update(dJ_dtheta, opt_state)\n",
    "        parameters = apply_updates(parameters, updates)\n",
    "\n",
    "        # Within episode book-keeping.\n",
    "        episode_return += new_timestep.reward\n",
    "        timestep = new_timestep\n",
    "        obs_tm1 = new_timestep.observation[0]\n",
    "        a_tm1 = a_t\n",
    "\n",
    "    # Experiment results tracking.\n",
    "    all_episode_returns.append(episode_return)\n",
    "\n",
    "# Plot learning curve.\n",
    "plot_learning_curve(all_episode_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reinf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
